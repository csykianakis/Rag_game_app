{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install langchain_community langchain gradio faiss-gpu","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import pipeline  \nfrom sentence_transformers import SentenceTransformer \nfrom langchain.text_splitter import RecursiveCharacterTextSplitter  \nfrom langchain_community.vectorstores import FAISS  \nfrom langchain_community.document_loaders import PyPDFLoader \nfrom langchain.chains import RetrievalQA  \nfrom langchain.embeddings import HuggingFaceEmbeddings\nimport gradio as gr\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom huggingface_hub import login\n\n\nlogin(\"...\")# replace 'your_huggingface_token' with your personal huggingface token (is free to get it)\n\nllm_pipeline = pipeline(\"text-generation\", model=\"meta-llama/Llama-3.2-3B-Instruct\", device_map=\"auto\")\n\ndef document_loader(file):\n    \"\"\"\n    Load a PDF document from the given file path.\n    Args:\n        file (str): Path to the PDF file to be loaded.\n    \"\"\"\n    loader = PyPDFLoader(file.name)\n    loaded_document = loader.load()\n    return loaded_document\n\ndef text_splitter(data):\n    \"\"\"\n    Split the loaded document into chunks for processing.\n    Args:\n        data (list): A list of documents to be split.\n    \"\"\"\n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=1000,  # Size of each chunk\n        chunk_overlap=50,  # Overlap between chunks\n        length_function=len,  # Function to calculate length\n    )\n    chunks = text_splitter.split_documents(data)  # Split the documents into chunks\n    return chunks  # Return the list of chunks\n\ndef get_embedding_model():\n    \"\"\"\n    Retrieve the embedding model for converting text to embeddings.\n\n    Returns:\n        HuggingFaceEmbeddings: The embedding model instance.\n    \"\"\"\n    return HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")  # Return the embedding model\n\ndef vector_database(chunks):\n    \"\"\"\n    Create a vector database from the text chunks using embeddings.\n\n    Args:\n        chunks (list): A list of text chunks to be embedded.\n\n    Returns:\n        FAISS: A FAISS vector store instance.\n    \"\"\"\n    embedding_model = get_embedding_model()  # Get the embedding model\n    vectordb = FAISS.from_documents(chunks, embedding_model)  # Create the vector database\n    return vectordb  # Return the vector database\n\ndef retriever(file):\n    \"\"\"\n    Create a retriever object for querying the vector database.\n\n    Args:\n        file (str): Path to the PDF file to be processed.\n\n    Returns:\n        FAISS: A FAISS retriever instance.\n    \"\"\"\n    splits = document_loader(file)  # Load the document\n    chunks = text_splitter(splits)  # Split the document into chunks\n    vectordb = vector_database(chunks)  # Create the vector database\n    retriever = vectordb.as_retriever()  # Convert to a retriever\n    return retriever  # Return the retriever\n\ndef retriever_qa(file, query):\n    \"\"\"\n    Perform a retrieval-based question-answering process.\n\n    Args:\n        file (str): Path to the PDF file to be processed.\n        query (str): The question to be answered.\n\n    Returns:\n        str: The answer to the query based on the document.\n    \"\"\"\n    retriever_obj = retriever(file)  # Initialize the retriever\n    docs = retriever_obj.get_relevant_documents(query)  # Retrieve relevant documents\n    context = \"\\n\".join([doc.page_content for doc in docs])  # Extract text from documents\n\n    # Debug: Print retrieved context\n    # print(f\"Retrieved Context:\\n{context}\")\n\n    # Ensure there's valid context\n    if not context.strip():\n        return \"No relevant information found in the document.\"  # Return if no context is found\n\n    # Generate response using the language model\n    prompt = f\"Answer the question based on the following context:\\n\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n    response = llm_pipeline(prompt, return_full_text=False, max_new_tokens=256, temperature=0.5)\n\n    return response[0]['generated_text']  # Return the generated answer\n\n# Create a Gradio interface for user interaction\nrag_application = gr.Interface(\n    fn=retriever_qa,  # Function to be called\n    allow_flagging=\"never\",  # Disable flagging\n    inputs=[\n        gr.File(label=\"Upload PDF File\", file_count=\"single\", file_types=['.pdf'], type=\"filepath\"),\n        gr.Textbox(label=\"Input Query\", lines=2, placeholder=\"Type your question here...\")\n    ],\n    outputs=gr.Textbox(label=\"Output\"),\n    title=\"RAG Chatbot\",  # Title of the application\n    description=\"Upload a PDF document and ask any question. The chatbot will try to answer using the provided document.\"\n)\n\n# Launch the Gradio application\nrag_application.launch(share=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T14:52:14.514049Z","iopub.execute_input":"2025-03-24T14:52:14.514346Z","iopub.status.idle":"2025-03-24T14:52:36.677996Z","shell.execute_reply.started":"2025-03-24T14:52:14.514325Z","shell.execute_reply":"2025-03-24T14:52:36.677291Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3359d6481034ceaa6636e235dee9a8c"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7860\n* Running on public URL: https://9da28826da3b317e85.gradio.live\n\nThis share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://9da28826da3b317e85.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}