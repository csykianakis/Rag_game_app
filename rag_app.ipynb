{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip install langchain_community langchain gradio faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T14:52:14.514346Z",
     "iopub.status.busy": "2025-03-24T14:52:14.514049Z",
     "iopub.status.idle": "2025-03-24T14:52:36.677996Z",
     "shell.execute_reply": "2025-03-24T14:52:36.677291Z",
     "shell.execute_reply.started": "2025-03-24T14:52:14.514325Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3359d6481034ceaa6636e235dee9a8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://9da28826da3b317e85.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://9da28826da3b317e85.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline  \n",
    "from sentence_transformers import SentenceTransformer \n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  \n",
    "from langchain_community.vectorstores import FAISS  \n",
    "from langchain_community.document_loaders import PyPDFLoader \n",
    "from langchain.chains import RetrievalQA  \n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import gradio as gr\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from huggingface_hub import login\n",
    "\n",
    "\n",
    "login(\"...\")# replace 'your_huggingface_token' with your personal huggingface token (is free to get it)\n",
    "\n",
    "llm_pipeline = pipeline(\"text-generation\", model=\"meta-llama/Llama-3.2-3B-Instruct\", device_map=\"auto\")\n",
    "\n",
    "def document_loader(file):\n",
    "    \"\"\"\n",
    "    Load a PDF document from the given file path.\n",
    "    Args:\n",
    "        file (str): Path to the PDF file to be loaded.\n",
    "    \"\"\"\n",
    "    loader = PyPDFLoader(file.name)\n",
    "    loaded_document = loader.load()\n",
    "    return loaded_document\n",
    "\n",
    "def text_splitter(data):\n",
    "    \"\"\"\n",
    "    Split the loaded document into chunks for processing.\n",
    "    Args:\n",
    "        data (list): A list of documents to be split.\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,  \n",
    "        chunk_overlap=50, \n",
    "        length_function=len,\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(data) \n",
    "    return chunks \n",
    "\n",
    "def get_embedding_model():\n",
    "    \"\"\"\n",
    "    Retrieve the embedding model for converting text to embeddings.\n",
    "\n",
    "    Returns:\n",
    "        HuggingFaceEmbeddings: The embedding model instance.\n",
    "    \"\"\"\n",
    "    return HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "def vector_database(chunks):\n",
    "    \"\"\"\n",
    "    Create a vector database from the text chunks using embeddings.\n",
    "    Args:\n",
    "        chunks (list): A list of text chunks to be embedded.\n",
    "    Returns:\n",
    "        FAISS: A FAISS vector store instance.\n",
    "    \"\"\"\n",
    "    embedding_model = get_embedding_model()  \n",
    "    vectordb = FAISS.from_documents(chunks, embedding_model) \n",
    "    return vectordb  \n",
    "\n",
    "def retriever(file):\n",
    "    \"\"\"\n",
    "    Create a retriever object for querying the vector database.\n",
    "    Args:\n",
    "        file (str): Path to the PDF file to be processed.\n",
    "    Returns:\n",
    "        FAISS: A FAISS retriever instance.\n",
    "    \"\"\"\n",
    "    splits = document_loader(file)\n",
    "    chunks = text_splitter(splits)  \n",
    "    vectordb = vector_database(chunks)  \n",
    "    retriever = vectordb.as_retriever()\n",
    "    return retriever  \n",
    "\n",
    "def retriever_qa(file, query):\n",
    "    \"\"\"\n",
    "    Perform a retrieval-based question-answering process.\n",
    "    Args:\n",
    "        file (str): Path to the PDF file to be processed.\n",
    "        query (str): The question to be answered.\n",
    "    Returns:\n",
    "        str: The answer to the query based on the document.\n",
    "    \"\"\"\n",
    "    retriever_obj = retriever(file) \n",
    "    docs = retriever_obj.get_relevant_documents(query)  \n",
    "    context = \"\\n\".join([doc.page_content for doc in docs])  \n",
    "\n",
    "    # Debug: Print retrieved context\n",
    "    # print(f\"Retrieved Context:\\n{context}\")\n",
    "\n",
    "    if not context.strip():\n",
    "        return \"No relevant information found in the document.\"  \n",
    "\n",
    "    prompt = f\"Answer the question based on the following context:\\n\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "    response = llm_pipeline(prompt, return_full_text=False, max_new_tokens=256, temperature=0.5)\n",
    "\n",
    "    return response[0]['generated_text'] \n",
    "\n",
    "rag_application = gr.Interface(\n",
    "    fn=retriever_qa,\n",
    "    allow_flagging=\"never\",\n",
    "    inputs=[\n",
    "        gr.File(label=\"Upload PDF File\", file_count=\"single\", file_types=['.pdf'], type=\"filepath\"),\n",
    "        gr.Textbox(label=\"Input Query\", lines=2, placeholder=\"Type your question here...\")\n",
    "    ],\n",
    "    outputs=gr.Textbox(label=\"Output\"),\n",
    "    title=\"RAG Chatbot\", \n",
    "    description=\"Upload a PDF document and ask any question. The chatbot will try to answer using the provided document.\"\n",
    ")\n",
    "\n",
    "# Launch the Gradio application\n",
    "rag_application.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
